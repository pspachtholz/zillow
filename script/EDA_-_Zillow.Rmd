---
title: "Exploratory Analysis - Zillow"
author: "Philipp Spachtholz"
output:
  html_document:
    fig_height: 4
    fig_width: 7
    theme: cosmo
---

### Welcome and good luck to you all at Zillow's Home Value prediction!

Here is a first exploratory analysis of the competition dataset.
We are provided with a list of  real estate properties in three counties (Los Angeles, Orange and Ventura, California) data in 2016.


Zillow provides a "Zestimate", which is an estimated property value.

Our task in this competition is to predict the the difference between the actual price and the estimate of the price (Zestimate). So, far so good. However, we don't have to predict a single price, but the price at 6 different time points (from October 2016 to December 2017).

The dataset consists of information about 2.9 million properties and is grouped into 2 files:
1) properties_2016.csv (containing information about the properties themselves)
2) train_2016.csv (containing information about the transcations)

Let's look at the dataset:

### Read in the data
```{r message=FALSE, warning=FALSE}
library(data.table)
library(dplyr)
library(ggplot2)
library(knitr)
library(stringr)
library(DT)
library(tidyr)

properties <- fread('../input/properties_2016.csv', showProgress = FALSE)
transactions <- fread('../input/train_2016.csv', showProgress = FALSE)
sample_submission <- fread('../input/sample_submission.csv')

```


```{r include=FALSE}
options(tibble.width = Inf)
```


Lets first have a look at these files:

### Peek at the dataset {.tabset}

#### Properties

```{r, result='asis'}
datatable(properties, style="bootstrap", class="table-condensed", options = list(dom = 'tp'))
```
#### Transactions

```{r, result='asis'}
datatable(transactions, style="bootstrap", class="table-condensed", options = list(dom = 'tp'))
```


#### Sample Submission

```{r, result='asis'}
datatable(sample_submission, style="bootstrap", class="table-condensed", options = list(dom = 'tp'))

```



### Outcome
To get a feel for the data let's first have a look at the distribution of our outcome (logerror), i.e. the difference in log(prediction)-log(saleprice)

```{r}
transactions %>% ggplot(aes(x=logerror)) + geom_histogram(bins=400, fill="red")+theme_bw()+theme(axis.title = element_text(size=16),axis.text = element_text(size=14))+ylab("Count")+coord_cartesian(x=c(-0.5,0.5))
```

### Missing values
We have seen many missing values in the data peeking. 
How many missing values are there for each feature?

```{r fig.height=15}
missing_values <- properties %>% summarize_each(funs(sum(is.na(.))/n()))

missing_values <- gather(missing_values, key="feature", value="missing_pct")
missing_values %>% ggplot(aes(x=reorder(feature,missing_pct),y=missing_pct)) +geom_bar(stat="identity",fill="red")+coord_flip()+theme_bw()

```



<br><br>

I am going to add more information later. So stay tuned.

And:

If the kernel helped you and you **upvote**, you make me happy :-)
